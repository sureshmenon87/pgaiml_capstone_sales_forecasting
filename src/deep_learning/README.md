# üß† Deep Learning (LSTM-based Sales Forecasting)

## üìå Overview

TASK 3 extends the sales forecasting system by introducing a **Deep Learning approach using LSTM (Long Short-Term Memory networks)** to model temporal dependencies in historical sales data.

While TASK 2 focused on **feature-based machine learning models (Linear, Random Forest, XGBoost)**, this task focuses on **sequence learning**, where the model learns directly from past values without explicit calendar features.

> ‚ö†Ô∏è Important Note
> This task is **not about outperforming ML at all costs**.
> It is about:
>
> - Understanding sequence models
> - Learning DL pipeline design
> - Comparing ML vs DL objectively

---

## üéØ Objectives

1. Convert historical daily sales into a time-series format
2. Build an LSTM model using sliding windows
3. Train and evaluate the LSTM model
4. Generate a **365-day future sales forecast**
5. Compare Deep Learning results with ML models from TASK 2

---

## üß© Why LSTM?

Traditional ML models require **manual feature engineering** (day, month, weekend, etc.).

LSTM:

- Learns **temporal dependencies implicitly**
- Captures trends, cycles, and short-term memory
- Is suitable for **sequential forecasting problems**

However:

- Requires more data
- Is slower to train
- Often **underperforms tree-based models on tabular data**

This project intentionally demonstrates this trade-off.

---

## üóÇÔ∏è Folder Structure (TASK 3)

```text
src/deep_learning/
‚îú‚îÄ‚îÄ README.md               # This document
‚îú‚îÄ‚îÄ data_prep.py            # Scaling + train/test preparation
‚îú‚îÄ‚îÄ sequence_builder.py     # Sliding window sequence creation
‚îú‚îÄ‚îÄ model.py                # LSTM architecture
‚îú‚îÄ‚îÄ train.py                # Training orchestration
‚îú‚îÄ‚îÄ evaluate.py             # RMSE calculation
‚îú‚îÄ‚îÄ predict.py              # (optional) inference helpers
‚îî‚îÄ‚îÄ forecast.py             # 1-year autoregressive forecast
```

Execution entry points:

```text
scripts/
‚îú‚îÄ‚îÄ run_train_dl.py
‚îî‚îÄ‚îÄ run_forecast_dl.py
```

---

## ‚öôÔ∏è Environment Requirements

### Python

- Python **3.10+**

### Dependencies

Ensure `requirements.txt` contains:

```txt
tensorflow>=2.15
numpy>=1.24
pandas>=2.0
scikit-learn>=1.3
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Verify TensorFlow:

```bash
python -c "import tensorflow as tf; print(tf.__version__)"
```

> CPU-only TensorFlow is sufficient. GPU is **not required**.

---

## üîπ Step 1 ‚Äì Data Preparation for LSTM

### Purpose

Prepare data in a format suitable for sequence models.

### Key Design Decisions

- Aggregate **global daily sales**
- Use **last 30 days ‚Üí predict next day**
- Apply **MinMax scaling**
- Strict **time-based split** (no shuffling)

### Core Logic

- Load `master_sales.csv`
- Aggregate by date
- Split:

  - ~85% train
  - ~15% test (last ~6 months)

- Build sliding windows

### Output Shapes

```text
X_train: (samples, 30, 1)
y_train: (samples,)
X_test : (samples, 30, 1)
y_test : (samples,)
```

üìÑ Implemented in:

- `data_prep.py`
- `sequence_builder.py`

> ‚ö†Ô∏è This module is **not run directly**.
> It is invoked internally during training and forecasting.

---

## üîπ Step 2 ‚Äì LSTM Model Architecture

### Model Design

```text
Input (30 timesteps)
   ‚Üì
LSTM (64 units, tanh)
   ‚Üì
Dense (1 output)
```

### Design Rationale

- Single LSTM layer (avoids overfitting)
- No dropout initially (baseline clarity)
- MSE loss (regression task)
- Adam optimizer

üìÑ Implemented in:
`model.py`

---

## üîπ Step 3 ‚Äì Model Training & Evaluation

### Runner Script

```bash
py -m scripts.run_train_dl
```

### What Happens Internally

1. Load & scale daily sales
2. Create sliding windows
3. Train LSTM for 20 epochs
4. Validate on last 6 months
5. Print RMSE (scaled)

### Example Output

```text
Epoch 20/20
loss: 0.0076 - val_loss: 0.0076
LSTM RMSE (scaled): 0.0871
```

### Interpretation

- Stable loss and validation loss
- No divergence ‚Üí no overfitting
- RMSE is on **scaled data**
- Used for **model sanity**, not direct ML comparison

üìÑ Implemented in:
`train.py`, `evaluate.py`

---

## üîπ Step 4 ‚Äì 1-Year Sales Forecast (LSTM)

### Forecast Strategy

- Retrain LSTM on **full historical data**
- Use last 30 days as seed
- Predict **one day at a time**
- Feed prediction back as input (autoregressive)
- Repeat for 365 days
- Inverse-scale predictions

### Runner Script

```bash
py -m scripts.run_forecast_dl
```

### Output File

```text
outputs/sales_forecast_lstm_next_year.csv
```

Example:

```csv
predicted_sales
6254.70
3332.51
1628.17
...
```

üìÑ Implemented in:
`forecast.py`

---

## ‚úÖ What TASK 3 Achieved

‚úî Built a full Deep Learning pipeline
‚úî Implemented sequence modeling correctly
‚úî Generated a 365-day forecast
‚úî Preserved clean modular architecture
‚úî Enabled ML vs DL comparison

---

# üìä ML vs DL ‚Äì Comparison & Learnings

## Model Performance Summary

| Approach | Model             | RMSE                                      |
| -------- | ----------------- | ----------------------------------------- |
| ML       | Linear Regression | 2258                                      |
| ML       | Random Forest     | 841                                       |
| ML       | **XGBoost**       | **666 (Best)**                            |
| DL       | LSTM              | Higher than XGBoost (scaled RMSE ‚âà 0.087) |

---

## Key Observations

### ‚úÖ Why XGBoost Performed Better

- Explicit calendar features
- Handles non-linear interactions well
- Strong bias-variance balance
- Ideal for structured/tabular data

### ‚ö†Ô∏è Why LSTM Didn‚Äôt Win

- No explicit weekday / seasonality features
- Limited data length
- More parameters to learn
- Autoregressive error accumulation

---

## Engineering Conclusion (Important)

> **For this problem, feature-based ML is superior to DL.**

This is a **correct and professional conclusion**, not a failure.

---

## When LSTM Would Win

- Much larger datasets
- Multiple correlated time series
- External signals (holidays, promotions)
- Long-term dependency dominance

---

## üß† Final Takeaway

| Aspect                  | Machine Learning | Deep Learning |
| ----------------------- | ---------------- | ------------- |
| Feature engineering     | Required         | Not required  |
| Training speed          | Fast             | Slower        |
| Interpretability        | High             | Low           |
| Performance (this task) | ‚úÖ Best          | ‚ö†Ô∏è Acceptable |
| Learning value          | Medium           | **High**      |
